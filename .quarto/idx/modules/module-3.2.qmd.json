{"title":"Module 3.2","markdown":{"yaml":{"title":"Module 3.2","subtitle":"Sampling and Uncertainty","format":{"html":{"code-link":true}},"highlight-style":"atom-one","execute":{"echo":true,"message":false,"warning":false}},"headingText":"Prework","containsRefs":false,"markdown":"\n\n::: {.callout-tip}\n\n- Install the following packages:\n  - `openintro`\n  - `infer`\n  - `tidymodels`\n:::\n\n# Overview\n\nIn this module, we will talk about the concepts of sampling and uncertainty. We will discuss the difference between a population and a sample, and how we can use a sample to make inferences about a population. We will also talk about the concept of uncertainty and how we can quantify it using confidence intervals. Finally, we will discuss the difference between parametric and nonparametric methods for calculating confidence intervals.\n\n{{< video https://youtu.be/z0Ry_3_qhDw?si=JgVxCwtpofL9drnC title='Sampling Distributions' >}}\n\n## Sampling\n\n**Sampling** is the act of selecting a subset of individuals, items, or data points from a larger population to estimate characteristics or metrics of the entire population. It can be contrasted with a **census**, which involves gathering information on every individual in the population. \n\nA **parameter** is a characteristic of a population whereas a **statistic** is a characteristic of a sample. So a sample statistic is used to estimate a population parameter.\n\nLet's imagine that we want to say something about the population of M&Ms and we take a sample of M&Ms from a bag. We calculate the proportion of blue M&Ms in our sample. This is our sample statistic. We can use this to estimate the proportion of blue M&Ms in the entire population of M&Ms.\n\n**Statistical inference** refers to this process of making a guess about a population using information from a sample. We make an inference when the population parameter is **unknown** and we use a sample statistic to estimate it. This is the tricky part of statistics--we are trying to make a guess about something we don't know. It is a challenging process that requires us to make some assumptions and to quantify our uncertainty in ways that we will talk about in a little bit.  \n\nNote that statistical inference is different from **causal inference**. Causal inference refers to when we are trying to determine if one thing is causally related another. There a lot of specialized methods for causal inference that we will not cover in this course. So even when we are talking about the relationship between two variables, we are just saying that there is a relationship, not that one causes the other.\n\n## Uncertainty\n\nLet's imagine that we take many samples from the same population and through that process come up with many estimates of a parameter of interest like a mean or the proportion of M&Ms that are blue. Due to random chance, one estimate from one sample can differ from another. This is called **sampling variability**.\n\nSampling variability entails that the process of taking many samples from our population would produce a **sampling distribution** of our **estimates**. This distribution would have a mean and a standard deviation. The standard deviation of this distribution has a special name--it is called the **standard error**.\n\nWhen we are making inferences, we want to both be able to generate an estimate *and* characterize our uncertainty with a *range* of possible estimates. We could do this by visualizing the sampling distribution of our estimates with a histogram or bar plot. But a more common way to do this is to calculate a **confidence interval** (CI).\n\nThe confidence interval is a range of values that we are confident contains the true population parameter. Confidence intervals are defined by **confidence levels** that represent the probability that the interval contains the true parameter. The most commonly used confidence interval is the 95% confidence interval. It says that we are 95% confident that the parameter value falls within the range given by that interval.\n\n## Parametric methods\n\nThere are a couple of ways that we can derive our confidence intervals. One is to calculate it from the sample statistic and the standard error. This is called a **parametric** method because it relies on assumptions about the distribution of the data.\n\nMore specifically, the parametric method takes advantage of the Central Limit Theorem (CLT) to estimate the confidence interval. The CLT says that the sampling distribution of the sample mean will be approximately normally distributed if the sample size is large enough.\n\nHere is the mathematical formula for deriving the CIs:\n\n$$CI = \\bar{x} \\pm Z \\left( \\frac{\\sigma}{\\sqrt{n}} \\right)$$\n\nHere $\\bar{x}$ is the sample mean, $Z$ is the Z-score corresponding to the desired level of confidence, $\\sigma$ is the population standard deviation, and $n$ is the sample size. \n\nThis part here represents the standard error: \n\n$$\\left( \\frac{\\sigma}{\\sqrt{n}} \\right)$$\n\nThe first intuition you want to walk away with here is that the bigger the standard error is, the bigger the CIs are going to be. Another intuition is that the bigger the sample size ($n$) is, the smaller the CIs are going to be because the spread of the sampling distribution gets narrower as the denominator gets bigger. \n\n## Nonparametric methods\n\nAnother way is to use a method called **bootstrapping**. It is a resampling method that involves taking many samples from the original sample and calculating the statistic of interest. The confidence interval is then calculated from the distribution of these statistics. This is a **nonparametric** method that uses the data to estimate the sampling distribution. It is referred to as a nonparametric method because it does not depend on assumptions about normality of the distribution of the data.\n\nHere is the general process for bootstrapping:\n\n1. Take a bootstrap sample - a random sample taken **with replacement** from the original sample, of the **same size** as the original sample;\n\n2. Calculate the bootstrap statistic - a statistic such as mean, median, proportion, slope, etc. computed on the bootstrap samples;\n\n3. Repeat steps (1) and (2) many times to create a bootstrap distribution - a distribution of bootstrap statistics;\n\n4. Calculate the bounds of the XX% confidence interval as the middle XX% \nof the bootstrap distribution (usually 95 percent confidence interval)\n\n## Russian survey example\n\nLet's look at an example to illustrate these concepts. In the `openintro` package, we can find a dataset that contains the results of a survey of 506 Russians about their beliefs about their country's interference in the 2016 US presidential election. The survey asked whether Russia tried to influence the election, and 40% of the respondents said that they did.\n\nFirst let's call the `openintro` package and view the dataset...\n\n```{r}\n#install.packages(\"openintro\")\nlibrary(openintro)\nlibrary(tidyverse)\n\nglimpse(russian_influence_on_us_election_2016)\n```\n\nNext, we will use `mutate()` to recode the qualitative variable as a numeric one...\n\n```{r}\nrussiaData <- russian_influence_on_us_election_2016 |> \n  mutate(try_influence = ifelse(influence_2016 == \"Did try\", 1, 0))\n```\n\nNow let's calculate the mean and standard deviation of the `try_influence` variable... \n\n```{r}\nrussiaData |>\n  summarize( \n          mean = mean(try_influence),\n          sd = sd(try_influence)\n  )\n```\n\nAnd finally let's draw a bar plot...\n\n```{r}\nggplot(russiaData, aes(x = try_influence)) +\n  geom_bar(fill = \"steelblue\", width = .75) +\n  labs(\n    title = \"Did Russia try to influence the U.S. election?\",\n    x = \"0 = 'No', 1 = 'Yes'\",\n    y = \"Frequncy\"\n  ) +\n  theme_minimal()\n```\n\nNow let's run the bootstrap using the `tidymodels` package. In this chunk, we pipe the `russiaData` dataset into the `specify()` function to specify the variable of interest. Then we pipe the output into the `generate()` function to generate 15000 bootstrap samples. Finally, we pipe the output into the `calculate()` function to calculate the mean of each bootstrap sample. Then we save the output to a new data frame called `boot_df` and `glimpse()` the data frame.\n\nBe sure to install `tidymodels` before running the code chunk. \n\n```{r}\n#| echo: true\n\nlibrary(tidymodels)\n\nset.seed(66)\nboot_dist <- russiaData |>\n  # specify the variable of interest\n  specify(response = try_influence) |>\n  # generate 15000 bootstrap samples\n  generate(reps = 15000, type = \"bootstrap\") |>\n  # calculate the mean of each bootstrap sample\n  calculate(stat = \"mean\")\n\nglimpse(boot_dist)\n```\n\nCalculate the confidence interval using the `get_ci()` function from the `infer` package which is part of the `tidymodels` suite. We will save the CI so that we can visualize it in the next step.\n\n```{r}\n#| echo: true \n\nci <- boot_dist |> get_ci(level = 0.95) \n\nci\n```\n\nWe interpret this confidence interval (CI) by stating that we are 95% confident that the proportion of Russians who believe that Russia interfered in the 2016 US election lies between the `lower_bound` and `upper_bound`. Or another way of putting it is that there is only a 5% chance that the true proportion lies outside of this interva. \n\nNow let's visualize the distribution and CIs using the `visualize()` and `shade_ci()` functions from the `infer` package. \n\n```{r}\nboot_dist |>\n  visualize() +\n  shade_ci(ci, color = \"red\", fill = NULL) +\n  labs( \n    title = \"Distribution of the Mean of the Bootstrap Samples\",\n    x = \"Mean\",\n    y = \"Count\"\n  ) +\n  theme_minimal() \n```\n\n::: {.callout-note}\nSince the `visualize()` function generates a `ggplot` object you can add layers to it like labels and a theme using the standard `ggplot2` syntax.\n:::\n\n\n\n\n","srcMarkdownNoYaml":"\n\n::: {.callout-tip}\n## Prework\n\n- Install the following packages:\n  - `openintro`\n  - `infer`\n  - `tidymodels`\n:::\n\n# Overview\n\nIn this module, we will talk about the concepts of sampling and uncertainty. We will discuss the difference between a population and a sample, and how we can use a sample to make inferences about a population. We will also talk about the concept of uncertainty and how we can quantify it using confidence intervals. Finally, we will discuss the difference between parametric and nonparametric methods for calculating confidence intervals.\n\n{{< video https://youtu.be/z0Ry_3_qhDw?si=JgVxCwtpofL9drnC title='Sampling Distributions' >}}\n\n## Sampling\n\n**Sampling** is the act of selecting a subset of individuals, items, or data points from a larger population to estimate characteristics or metrics of the entire population. It can be contrasted with a **census**, which involves gathering information on every individual in the population. \n\nA **parameter** is a characteristic of a population whereas a **statistic** is a characteristic of a sample. So a sample statistic is used to estimate a population parameter.\n\nLet's imagine that we want to say something about the population of M&Ms and we take a sample of M&Ms from a bag. We calculate the proportion of blue M&Ms in our sample. This is our sample statistic. We can use this to estimate the proportion of blue M&Ms in the entire population of M&Ms.\n\n**Statistical inference** refers to this process of making a guess about a population using information from a sample. We make an inference when the population parameter is **unknown** and we use a sample statistic to estimate it. This is the tricky part of statistics--we are trying to make a guess about something we don't know. It is a challenging process that requires us to make some assumptions and to quantify our uncertainty in ways that we will talk about in a little bit.  \n\nNote that statistical inference is different from **causal inference**. Causal inference refers to when we are trying to determine if one thing is causally related another. There a lot of specialized methods for causal inference that we will not cover in this course. So even when we are talking about the relationship between two variables, we are just saying that there is a relationship, not that one causes the other.\n\n## Uncertainty\n\nLet's imagine that we take many samples from the same population and through that process come up with many estimates of a parameter of interest like a mean or the proportion of M&Ms that are blue. Due to random chance, one estimate from one sample can differ from another. This is called **sampling variability**.\n\nSampling variability entails that the process of taking many samples from our population would produce a **sampling distribution** of our **estimates**. This distribution would have a mean and a standard deviation. The standard deviation of this distribution has a special name--it is called the **standard error**.\n\nWhen we are making inferences, we want to both be able to generate an estimate *and* characterize our uncertainty with a *range* of possible estimates. We could do this by visualizing the sampling distribution of our estimates with a histogram or bar plot. But a more common way to do this is to calculate a **confidence interval** (CI).\n\nThe confidence interval is a range of values that we are confident contains the true population parameter. Confidence intervals are defined by **confidence levels** that represent the probability that the interval contains the true parameter. The most commonly used confidence interval is the 95% confidence interval. It says that we are 95% confident that the parameter value falls within the range given by that interval.\n\n## Parametric methods\n\nThere are a couple of ways that we can derive our confidence intervals. One is to calculate it from the sample statistic and the standard error. This is called a **parametric** method because it relies on assumptions about the distribution of the data.\n\nMore specifically, the parametric method takes advantage of the Central Limit Theorem (CLT) to estimate the confidence interval. The CLT says that the sampling distribution of the sample mean will be approximately normally distributed if the sample size is large enough.\n\nHere is the mathematical formula for deriving the CIs:\n\n$$CI = \\bar{x} \\pm Z \\left( \\frac{\\sigma}{\\sqrt{n}} \\right)$$\n\nHere $\\bar{x}$ is the sample mean, $Z$ is the Z-score corresponding to the desired level of confidence, $\\sigma$ is the population standard deviation, and $n$ is the sample size. \n\nThis part here represents the standard error: \n\n$$\\left( \\frac{\\sigma}{\\sqrt{n}} \\right)$$\n\nThe first intuition you want to walk away with here is that the bigger the standard error is, the bigger the CIs are going to be. Another intuition is that the bigger the sample size ($n$) is, the smaller the CIs are going to be because the spread of the sampling distribution gets narrower as the denominator gets bigger. \n\n## Nonparametric methods\n\nAnother way is to use a method called **bootstrapping**. It is a resampling method that involves taking many samples from the original sample and calculating the statistic of interest. The confidence interval is then calculated from the distribution of these statistics. This is a **nonparametric** method that uses the data to estimate the sampling distribution. It is referred to as a nonparametric method because it does not depend on assumptions about normality of the distribution of the data.\n\nHere is the general process for bootstrapping:\n\n1. Take a bootstrap sample - a random sample taken **with replacement** from the original sample, of the **same size** as the original sample;\n\n2. Calculate the bootstrap statistic - a statistic such as mean, median, proportion, slope, etc. computed on the bootstrap samples;\n\n3. Repeat steps (1) and (2) many times to create a bootstrap distribution - a distribution of bootstrap statistics;\n\n4. Calculate the bounds of the XX% confidence interval as the middle XX% \nof the bootstrap distribution (usually 95 percent confidence interval)\n\n## Russian survey example\n\nLet's look at an example to illustrate these concepts. In the `openintro` package, we can find a dataset that contains the results of a survey of 506 Russians about their beliefs about their country's interference in the 2016 US presidential election. The survey asked whether Russia tried to influence the election, and 40% of the respondents said that they did.\n\nFirst let's call the `openintro` package and view the dataset...\n\n```{r}\n#install.packages(\"openintro\")\nlibrary(openintro)\nlibrary(tidyverse)\n\nglimpse(russian_influence_on_us_election_2016)\n```\n\nNext, we will use `mutate()` to recode the qualitative variable as a numeric one...\n\n```{r}\nrussiaData <- russian_influence_on_us_election_2016 |> \n  mutate(try_influence = ifelse(influence_2016 == \"Did try\", 1, 0))\n```\n\nNow let's calculate the mean and standard deviation of the `try_influence` variable... \n\n```{r}\nrussiaData |>\n  summarize( \n          mean = mean(try_influence),\n          sd = sd(try_influence)\n  )\n```\n\nAnd finally let's draw a bar plot...\n\n```{r}\nggplot(russiaData, aes(x = try_influence)) +\n  geom_bar(fill = \"steelblue\", width = .75) +\n  labs(\n    title = \"Did Russia try to influence the U.S. election?\",\n    x = \"0 = 'No', 1 = 'Yes'\",\n    y = \"Frequncy\"\n  ) +\n  theme_minimal()\n```\n\nNow let's run the bootstrap using the `tidymodels` package. In this chunk, we pipe the `russiaData` dataset into the `specify()` function to specify the variable of interest. Then we pipe the output into the `generate()` function to generate 15000 bootstrap samples. Finally, we pipe the output into the `calculate()` function to calculate the mean of each bootstrap sample. Then we save the output to a new data frame called `boot_df` and `glimpse()` the data frame.\n\nBe sure to install `tidymodels` before running the code chunk. \n\n```{r}\n#| echo: true\n\nlibrary(tidymodels)\n\nset.seed(66)\nboot_dist <- russiaData |>\n  # specify the variable of interest\n  specify(response = try_influence) |>\n  # generate 15000 bootstrap samples\n  generate(reps = 15000, type = \"bootstrap\") |>\n  # calculate the mean of each bootstrap sample\n  calculate(stat = \"mean\")\n\nglimpse(boot_dist)\n```\n\nCalculate the confidence interval using the `get_ci()` function from the `infer` package which is part of the `tidymodels` suite. We will save the CI so that we can visualize it in the next step.\n\n```{r}\n#| echo: true \n\nci <- boot_dist |> get_ci(level = 0.95) \n\nci\n```\n\nWe interpret this confidence interval (CI) by stating that we are 95% confident that the proportion of Russians who believe that Russia interfered in the 2016 US election lies between the `lower_bound` and `upper_bound`. Or another way of putting it is that there is only a 5% chance that the true proportion lies outside of this interva. \n\nNow let's visualize the distribution and CIs using the `visualize()` and `shade_ci()` functions from the `infer` package. \n\n```{r}\nboot_dist |>\n  visualize() +\n  shade_ci(ci, color = \"red\", fill = NULL) +\n  labs( \n    title = \"Distribution of the Mean of the Bootstrap Samples\",\n    x = \"Mean\",\n    y = \"Count\"\n  ) +\n  theme_minimal() \n```\n\n::: {.callout-note}\nSince the `visualize()` function generates a `ggplot` object you can add layers to it like labels and a theme using the standard `ggplot2` syntax.\n:::\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"highlight-style":"atom-one","output-file":"module-3.2.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.533","editor":"source","theme":{"light":"cosmo","dark":"cyborg"},"mainfont":"Atkinson Hyperlegible","code-copy":true,"title":"Module 3.2","subtitle":"Sampling and Uncertainty"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}