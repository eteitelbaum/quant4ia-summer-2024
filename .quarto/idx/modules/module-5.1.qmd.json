{"title":"Module 5.1","markdown":{"yaml":{"title":"Module 5.1","subtitle":"Linear Regression","format":{"html":{"code-link":true}},"highlight-style":"atom-one","execute":{"echo":false,"message":false,"warning":false}},"headingText":"Overview","containsRefs":false,"markdown":"\n\n\n{{< video https://youtu.be/dLc-lfEEYss?si=-Zelb-HBAP-MPvnF title='Linear Regression 1' >}}\n\n## Modeling \n\nModels are a common feature of modern social scientific analysis. A **model** is a simplified representation of reality that we use to make predictions or to explain relationships between variables. We use models to help us address important questions like \"is oil wealth associated with regime type?\" or \"where is violence most likely to happen in country X during their next election?\" We use models to help explain the relationship between variables and to make predictions.\n\nAt root, models are based on the idea that we can represent relationships between variables using **functions**. A function is a mathematical concept that describes the relationship between an output and one or more inputs. In the context of social science, we often think of functions as describing the relationship between a **dependent variable** (the thing we are trying to explain or predict) and one or more **independent variables** (the things we think might help us explain or predict the outcome).\n\n::: {.callout-note}\nA word about social science terminology. In social science, we often use the term **dependent variable** to refer to the outcome variable and **independent variable** to refer to the explanatory variables. But you will hear many other terms used to refer to these concepts. For example, in statistics, the dependent variable is often called the **response variable**, the **outcome variable** or the **Y variable** while the independent variable is rreferred to as **explanatory variables**, or **predictors**. We will use these terms interchangeably.\n:::\n\nThe idea behind a function is that we can plug in the inputs and receive back the output. For example, the formula $y = 3x + 7$ is a function with input $x$ and output $y$. If $x$ is $5$, $y$ is $22$. In this case, $y = 3 \\times 5 + 7 = 22$. In social science models, $x$ and $y$ are going to be variables like GDP per capita and democracy, or oil wealth and regime type.\n\nTypically, we are going to use models when we have **observational data**. Observational data is data that we have collected from the world around us and can be contrasted with **experimental data** which is data that we have collected from a controlled experiment. In the social sciences, we often use observational data because it is difficult to conduct controlled experiments on social phenomena. For example, we can't randomly assign countries to have different levels of GDP per capita and then observe the effect on democracy. Instead, we have to use observational data to try to understand the relationship between GDP per capita and democracy.\n\n## Linear Model with Single Predictor\n\nLet's dive right in and estimate a linear model: \n\n$$Y = a + bX$$\n\nIn this abstract equation, $Y$ is the outcome variable, $X$ is the explanatory variable, $a$ is the intercept, and $b$ is the slope of the line. The equation tells us that the predicted value of $Y$ is equal to the intercept plus the slope times the value of $X$. This kind of equation is sometimes referred to as a **bivariate regression** model because it has only one explanatory variable.\n\nOur goal here is to estimate the relationship between GDP per capita and democracy. Our expectation, based on modernization theroy, is that as GDP per capita increases, democracy will also increase. We can start by wrangling some data from the Varieties of Democracy dataset.\n\n```{r}\n#| label: data\n#| echo: true\n\nlibrary(tidyverse)\nlibrary(vdemdata)\n\nmodel_data <- vdem |> \n  filter(year == 2019) |> \n  select(\n    country = country_name, \n    lib_dem = v2x_libdem, \n    wealth = e_gdppc\n    ) |>\n  mutate(log_wealth = log(wealth)) \n\nglimpse(model_data)\n```\n\nHere we have a dataset with three variables from 2019: `country`, `lib_dem`, `wealth`, and `log_wealth`. `lib_dem` is the liberal democracy index from V-Dem, `wealth` is GDP per capita, and `log_wealth` is the natural log of GDP per capita. We are going to use `log_wealth` as our explanatory variable and `lib_dem` as our outcome variable.\n\nWe want to estimate the following equation: \n\n$$lib\\_dem = a + b \\times log\\_wealth$$\n\n$a$ is our predicted level of democracy when the log of GDP per capita is 0. $b$ is the predicted change in $Y$ (liberay democracy) **associated with** a one unit change in log GDP per capita.\n\nThe way we do that in R is to use the `lm()` function as follows:\n\n```{r}\n#| label: model\n#| echo: true\n\nlm <- lm(lib_dem ~ log_wealth, data = model_data)\n\nsummary(lm)\n```\n\nNotice that we use the `~` symbol to indicate that `lib_dem` is the outcome variable and `log_wealth` is the explanatory variable. Even though this is an equation, we cannot use `=` sign because in R `=` is an assignment operator just like `<-`. The `lm()` function estimates the coefficients of the linear model. The `summary()` function gives us a summary of the model including the coefficients, the standard errors, the t-statistics, and the p-values.\n\nThe output of the model suggests that the relationship between GDP per capita and democracy is positive and statistically significant. Based on these results, we can \"fill in\" our equation as follows:\n\n$$\\widehat{lib\\_dem}_{i} = 0.13 + 0.12 * {log\\_wealth}_{i}$$\n\nHere we see that the **intercept** is 0.13 and the **slope** of `log_wealth` is 0.12. We can interpret the slope as follows: for every one unit increase in the natural log of GDP per capita, democracy increases by 0.12 units. Notice that the coefficient for log wealth is also statistically significant. The p-value is less than 0.05, which means that we can reject the null hypothesis that the coefficient is equal to zero.\n\nOur equation actually defines a line. Let's go ahead and look at what that line looks like when we plot it against a scatter plot of wealth and democracy. We can do that by specifying `geom_smooth(method = \"lm\")` in the `ggplot()` function. We set `se = FALSE` to remove the confidence interval around the line.\n\n```{r}\n#| label: model-plot\n#| echo: true\n\nggplot(model_data, aes(x = log_wealth, y = lib_dem)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"#E48957\", se = FALSE) +\n  labs(x = \"GDP per capita\", y = \"Liberal Democracy Index\") +\n  theme_bw()\n```\n\nThe orange line you see in the plot is the line that we estimated using the linear model. The line represents the predicted relationship between GDP per capita and democracy. The intercept of the line is 0.13, which means that when GDP per capita is zero, democracy is 0.13. This is a theoretical point because GDP per capita cannot be zero in the real world.\n\nNow, let's visualize the interpretation of the slope with a couple of arrows. With a one-unit change of the natural log of GDP per capita... \n\n```{r}\n#| label: interpretation1\n#| echo: false\n\nggplot(model_data, aes(x = log_wealth, y = lib_dem)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"#E48957\", se = FALSE) +\n  labs(x = \"GPD per capita\", y = \"Liberal Democracy Index\") +\n  geom_segment(aes(x = 2, xend= 3, y=.37, yend=.37), colour=\"darkblue\", linewidth=1.5, arrow = arrow(length = unit(0.5, \"cm\"))) +\n  theme_bw()\n```\n\ndemocracy increases by 0.12 units...\n\n```{r}\n#| label: interpretation2\n#| echo: false\n\nggplot(model_data, aes(x = log_wealth, y = lib_dem)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"#E48957\", se = FALSE) +\n  labs(x = \"GPD per capita\", y = \"Liberal Democracy Index\") +\n  geom_segment(aes(x = 2, xend= 3, y=.37, yend=.37), colour=\"darkblue\", linewidth=1.5) +\n  geom_segment(aes(x = 3, xend= 3, y=.37, yend=.49), colour=\"darkblue\", linewidth=1.5, arrow = arrow(length = unit(0.5, \"cm\"))) +\n  geom_text(x=3.2, y=.43, label=\"0.12\", color=\"darkblue\", size=4) +\n  theme_bw()\n```\n\nThis is the slope of the line that we estimated. The second arrow shows the effect of a two unit increase in the natural log of GDP per capita on democracy.\n\n:::{.callout-warning}\nIs this the **causal** effect of GDP per capita on liberal democracy? No, it is the **association** between GDP per capita and democracy. To identify causality we need other methods that are beyond the scope of this course).\n:::\n\n## How is the \"best\" line drawn?\n\n{{< video https://youtu.be/peNRqkfukYY?si=DbXLOcaUcQpf000b title='The Cost Function' >}}\n\nHow do we get the \"best\" values for the slope and intercept? When we estimate a least squares regression, it is looking for the line that minimizes sum of squared **residuals**. Residuals are the difference between the observed value and the predicted value, e.g. $e_i = y_i - \\hat{y}_i$. The least squares regression line minimizes $\\sum_{i = 1}^n e_i^2$.\n\nLet's look at a simple example where we have three data points:\n\n```{r}\n#| label: best-line4\n\n# create data\ndat <- tibble(\n    x = c(1, 2, 3),\n    y = c(1, 2, 3)\n)\n\nggplot(dat, aes(y=y, x=x)) +\n  geom_point(size=3, color=\"darkblue\") +\n  xlim(0, 4) + ylim(0,4) +\n  theme_bw()\n```\n\nHere the best line is $\\hat{Y} = 0 + 1*X$. This line goes perfectly through the points and the sum of squared residuals would be zero. \n\n```{r}\n#| label: best-line5\n\nggplot(dat, aes(y=y, x=x)) +\n  geom_point(size=3, color=\"darkblue\") +\n  xlim(0, 4) + ylim(0,4) +\n  geom_segment(x=0, y=0, xend=4, yend=4, color=\"darkorange\") +\n  theme_bw()\n```\n\nBut let's pretend for a second that we have a line $y = 0 + 0*X$. \n\n```{r}\n#| label: best-line7\n\n ggplot(dat, aes(y=y, x=x)) +\n  geom_point(size=3, color=\"darkblue\") +\n  xlim(0, 4) + ylim(0,4) +\n  geom_segment(x=0, y=0, xend=4, yend=0, color=\"black\") +\n  theme_bw()\n```\n\nWhat is the sum of squared residuals for it? Here the residuals are $1^2 + 2^2 + 3^2 = 14$.\n\n```{r}\n#| label: best-line9\n#| echo: true\n\n(1-0)^2 + (2-0)^2 + (3-0)^2\n```\n\nAnd how about a line defined by the equation $y = 0 + 2*X$? \n\n```{r}\n#| label: best-line10\n\n ggplot(dat, aes(y=y, x=x)) +\n  geom_point(size=3, color=\"darkblue\") +\n  xlim(0, 4) + ylim(0,8) +\n  geom_segment(x=0, y=0, xend=4, yend=8, color=\"black\") +\n  theme_bw()\n```\n\nWhat is sum of squared residuals for $y = 0 + 2*X$? It would again be 14: $1^2 + 2^2 + 3^2 = 14$.\n\n```{r}\n#| label: best-line12\n#| echo: true\n\n(1-2)^2 + (2-4)^2 + (3-6)^2\n```\n\nAnd what is sum of squared residuals for $y = 0 + -1*X$?\n\n```{r}\n#| label: best-line13\n\n ggplot(dat, aes(y=y, x=x)) +\n  geom_point(size=3, color=\"darkblue\") +\n  xlim(0, 4) + ylim(-4,4) +\n  geom_segment(x=0, y=0, xend=4, yend=-4, color=\"black\") +\n  theme_bw()\n```\n\nThis time it would be $1^2 + 2^2 + 3^2 = 56$.\n\n```{r}\n#| label: best-line15\n#| echo: true\n\n(1+1)^2 + (2+2)^2 + (3+3)^2\n```\n\nWe can imagine an algorithm that would try all possible values of $b$ and calculate the sum of squared residuals for each of them. The value of $b$ that minimizes the sum of squared residuals is the one that we are looking for. In machine learning, this algorithm is commonly referred to as the **cost function**. We can visualize the cost function for our simple example with the following plot:\n\n```{r}\n#| label: cost-function\n\nsse <- tibble(\n          b=c(-2, -1, 0, 1, 2, 3, 4), \n          c=c(81, 56, 14, 0, 14, 56, 81)\n          )\n\nggplot(sse, aes(y=c, x=b)) +\n    geom_point(size=3, color=\"darkred\") +\n    labs(\n    x = \"Slope (b)\",\n    y = \"Sum of Squared Residuals\"\n  ) +\n  theme_bw()\n```\n\n\n","srcMarkdownNoYaml":"\n\n## Overview\n\n{{< video https://youtu.be/dLc-lfEEYss?si=-Zelb-HBAP-MPvnF title='Linear Regression 1' >}}\n\n## Modeling \n\nModels are a common feature of modern social scientific analysis. A **model** is a simplified representation of reality that we use to make predictions or to explain relationships between variables. We use models to help us address important questions like \"is oil wealth associated with regime type?\" or \"where is violence most likely to happen in country X during their next election?\" We use models to help explain the relationship between variables and to make predictions.\n\nAt root, models are based on the idea that we can represent relationships between variables using **functions**. A function is a mathematical concept that describes the relationship between an output and one or more inputs. In the context of social science, we often think of functions as describing the relationship between a **dependent variable** (the thing we are trying to explain or predict) and one or more **independent variables** (the things we think might help us explain or predict the outcome).\n\n::: {.callout-note}\nA word about social science terminology. In social science, we often use the term **dependent variable** to refer to the outcome variable and **independent variable** to refer to the explanatory variables. But you will hear many other terms used to refer to these concepts. For example, in statistics, the dependent variable is often called the **response variable**, the **outcome variable** or the **Y variable** while the independent variable is rreferred to as **explanatory variables**, or **predictors**. We will use these terms interchangeably.\n:::\n\nThe idea behind a function is that we can plug in the inputs and receive back the output. For example, the formula $y = 3x + 7$ is a function with input $x$ and output $y$. If $x$ is $5$, $y$ is $22$. In this case, $y = 3 \\times 5 + 7 = 22$. In social science models, $x$ and $y$ are going to be variables like GDP per capita and democracy, or oil wealth and regime type.\n\nTypically, we are going to use models when we have **observational data**. Observational data is data that we have collected from the world around us and can be contrasted with **experimental data** which is data that we have collected from a controlled experiment. In the social sciences, we often use observational data because it is difficult to conduct controlled experiments on social phenomena. For example, we can't randomly assign countries to have different levels of GDP per capita and then observe the effect on democracy. Instead, we have to use observational data to try to understand the relationship between GDP per capita and democracy.\n\n## Linear Model with Single Predictor\n\nLet's dive right in and estimate a linear model: \n\n$$Y = a + bX$$\n\nIn this abstract equation, $Y$ is the outcome variable, $X$ is the explanatory variable, $a$ is the intercept, and $b$ is the slope of the line. The equation tells us that the predicted value of $Y$ is equal to the intercept plus the slope times the value of $X$. This kind of equation is sometimes referred to as a **bivariate regression** model because it has only one explanatory variable.\n\nOur goal here is to estimate the relationship between GDP per capita and democracy. Our expectation, based on modernization theroy, is that as GDP per capita increases, democracy will also increase. We can start by wrangling some data from the Varieties of Democracy dataset.\n\n```{r}\n#| label: data\n#| echo: true\n\nlibrary(tidyverse)\nlibrary(vdemdata)\n\nmodel_data <- vdem |> \n  filter(year == 2019) |> \n  select(\n    country = country_name, \n    lib_dem = v2x_libdem, \n    wealth = e_gdppc\n    ) |>\n  mutate(log_wealth = log(wealth)) \n\nglimpse(model_data)\n```\n\nHere we have a dataset with three variables from 2019: `country`, `lib_dem`, `wealth`, and `log_wealth`. `lib_dem` is the liberal democracy index from V-Dem, `wealth` is GDP per capita, and `log_wealth` is the natural log of GDP per capita. We are going to use `log_wealth` as our explanatory variable and `lib_dem` as our outcome variable.\n\nWe want to estimate the following equation: \n\n$$lib\\_dem = a + b \\times log\\_wealth$$\n\n$a$ is our predicted level of democracy when the log of GDP per capita is 0. $b$ is the predicted change in $Y$ (liberay democracy) **associated with** a one unit change in log GDP per capita.\n\nThe way we do that in R is to use the `lm()` function as follows:\n\n```{r}\n#| label: model\n#| echo: true\n\nlm <- lm(lib_dem ~ log_wealth, data = model_data)\n\nsummary(lm)\n```\n\nNotice that we use the `~` symbol to indicate that `lib_dem` is the outcome variable and `log_wealth` is the explanatory variable. Even though this is an equation, we cannot use `=` sign because in R `=` is an assignment operator just like `<-`. The `lm()` function estimates the coefficients of the linear model. The `summary()` function gives us a summary of the model including the coefficients, the standard errors, the t-statistics, and the p-values.\n\nThe output of the model suggests that the relationship between GDP per capita and democracy is positive and statistically significant. Based on these results, we can \"fill in\" our equation as follows:\n\n$$\\widehat{lib\\_dem}_{i} = 0.13 + 0.12 * {log\\_wealth}_{i}$$\n\nHere we see that the **intercept** is 0.13 and the **slope** of `log_wealth` is 0.12. We can interpret the slope as follows: for every one unit increase in the natural log of GDP per capita, democracy increases by 0.12 units. Notice that the coefficient for log wealth is also statistically significant. The p-value is less than 0.05, which means that we can reject the null hypothesis that the coefficient is equal to zero.\n\nOur equation actually defines a line. Let's go ahead and look at what that line looks like when we plot it against a scatter plot of wealth and democracy. We can do that by specifying `geom_smooth(method = \"lm\")` in the `ggplot()` function. We set `se = FALSE` to remove the confidence interval around the line.\n\n```{r}\n#| label: model-plot\n#| echo: true\n\nggplot(model_data, aes(x = log_wealth, y = lib_dem)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"#E48957\", se = FALSE) +\n  labs(x = \"GDP per capita\", y = \"Liberal Democracy Index\") +\n  theme_bw()\n```\n\nThe orange line you see in the plot is the line that we estimated using the linear model. The line represents the predicted relationship between GDP per capita and democracy. The intercept of the line is 0.13, which means that when GDP per capita is zero, democracy is 0.13. This is a theoretical point because GDP per capita cannot be zero in the real world.\n\nNow, let's visualize the interpretation of the slope with a couple of arrows. With a one-unit change of the natural log of GDP per capita... \n\n```{r}\n#| label: interpretation1\n#| echo: false\n\nggplot(model_data, aes(x = log_wealth, y = lib_dem)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"#E48957\", se = FALSE) +\n  labs(x = \"GPD per capita\", y = \"Liberal Democracy Index\") +\n  geom_segment(aes(x = 2, xend= 3, y=.37, yend=.37), colour=\"darkblue\", linewidth=1.5, arrow = arrow(length = unit(0.5, \"cm\"))) +\n  theme_bw()\n```\n\ndemocracy increases by 0.12 units...\n\n```{r}\n#| label: interpretation2\n#| echo: false\n\nggplot(model_data, aes(x = log_wealth, y = lib_dem)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"#E48957\", se = FALSE) +\n  labs(x = \"GPD per capita\", y = \"Liberal Democracy Index\") +\n  geom_segment(aes(x = 2, xend= 3, y=.37, yend=.37), colour=\"darkblue\", linewidth=1.5) +\n  geom_segment(aes(x = 3, xend= 3, y=.37, yend=.49), colour=\"darkblue\", linewidth=1.5, arrow = arrow(length = unit(0.5, \"cm\"))) +\n  geom_text(x=3.2, y=.43, label=\"0.12\", color=\"darkblue\", size=4) +\n  theme_bw()\n```\n\nThis is the slope of the line that we estimated. The second arrow shows the effect of a two unit increase in the natural log of GDP per capita on democracy.\n\n:::{.callout-warning}\nIs this the **causal** effect of GDP per capita on liberal democracy? No, it is the **association** between GDP per capita and democracy. To identify causality we need other methods that are beyond the scope of this course).\n:::\n\n## How is the \"best\" line drawn?\n\n{{< video https://youtu.be/peNRqkfukYY?si=DbXLOcaUcQpf000b title='The Cost Function' >}}\n\nHow do we get the \"best\" values for the slope and intercept? When we estimate a least squares regression, it is looking for the line that minimizes sum of squared **residuals**. Residuals are the difference between the observed value and the predicted value, e.g. $e_i = y_i - \\hat{y}_i$. The least squares regression line minimizes $\\sum_{i = 1}^n e_i^2$.\n\nLet's look at a simple example where we have three data points:\n\n```{r}\n#| label: best-line4\n\n# create data\ndat <- tibble(\n    x = c(1, 2, 3),\n    y = c(1, 2, 3)\n)\n\nggplot(dat, aes(y=y, x=x)) +\n  geom_point(size=3, color=\"darkblue\") +\n  xlim(0, 4) + ylim(0,4) +\n  theme_bw()\n```\n\nHere the best line is $\\hat{Y} = 0 + 1*X$. This line goes perfectly through the points and the sum of squared residuals would be zero. \n\n```{r}\n#| label: best-line5\n\nggplot(dat, aes(y=y, x=x)) +\n  geom_point(size=3, color=\"darkblue\") +\n  xlim(0, 4) + ylim(0,4) +\n  geom_segment(x=0, y=0, xend=4, yend=4, color=\"darkorange\") +\n  theme_bw()\n```\n\nBut let's pretend for a second that we have a line $y = 0 + 0*X$. \n\n```{r}\n#| label: best-line7\n\n ggplot(dat, aes(y=y, x=x)) +\n  geom_point(size=3, color=\"darkblue\") +\n  xlim(0, 4) + ylim(0,4) +\n  geom_segment(x=0, y=0, xend=4, yend=0, color=\"black\") +\n  theme_bw()\n```\n\nWhat is the sum of squared residuals for it? Here the residuals are $1^2 + 2^2 + 3^2 = 14$.\n\n```{r}\n#| label: best-line9\n#| echo: true\n\n(1-0)^2 + (2-0)^2 + (3-0)^2\n```\n\nAnd how about a line defined by the equation $y = 0 + 2*X$? \n\n```{r}\n#| label: best-line10\n\n ggplot(dat, aes(y=y, x=x)) +\n  geom_point(size=3, color=\"darkblue\") +\n  xlim(0, 4) + ylim(0,8) +\n  geom_segment(x=0, y=0, xend=4, yend=8, color=\"black\") +\n  theme_bw()\n```\n\nWhat is sum of squared residuals for $y = 0 + 2*X$? It would again be 14: $1^2 + 2^2 + 3^2 = 14$.\n\n```{r}\n#| label: best-line12\n#| echo: true\n\n(1-2)^2 + (2-4)^2 + (3-6)^2\n```\n\nAnd what is sum of squared residuals for $y = 0 + -1*X$?\n\n```{r}\n#| label: best-line13\n\n ggplot(dat, aes(y=y, x=x)) +\n  geom_point(size=3, color=\"darkblue\") +\n  xlim(0, 4) + ylim(-4,4) +\n  geom_segment(x=0, y=0, xend=4, yend=-4, color=\"black\") +\n  theme_bw()\n```\n\nThis time it would be $1^2 + 2^2 + 3^2 = 56$.\n\n```{r}\n#| label: best-line15\n#| echo: true\n\n(1+1)^2 + (2+2)^2 + (3+3)^2\n```\n\nWe can imagine an algorithm that would try all possible values of $b$ and calculate the sum of squared residuals for each of them. The value of $b$ that minimizes the sum of squared residuals is the one that we are looking for. In machine learning, this algorithm is commonly referred to as the **cost function**. We can visualize the cost function for our simple example with the following plot:\n\n```{r}\n#| label: cost-function\n\nsse <- tibble(\n          b=c(-2, -1, 0, 1, 2, 3, 4), \n          c=c(81, 56, 14, 0, 14, 56, 81)\n          )\n\nggplot(sse, aes(y=c, x=b)) +\n    geom_point(size=3, color=\"darkred\") +\n    labs(\n    x = \"Slope (b)\",\n    y = \"Sum of Squared Residuals\"\n  ) +\n  theme_bw()\n```\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"highlight-style":"atom-one","output-file":"module-5.1.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.533","editor":"source","theme":{"light":"cosmo","dark":"cyborg"},"mainfont":"Atkinson Hyperlegible","code-copy":true,"title":"Module 5.1","subtitle":"Linear Regression"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}