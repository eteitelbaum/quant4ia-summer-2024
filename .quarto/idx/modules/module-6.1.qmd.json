{"title":"Module 6.1","markdown":{"yaml":{"title":"Module 6.1","subtitle":"Logistic Regression","format":{"html":{"code-link":true}},"highlight-style":"atom-one","execute":{"echo":true,"message":false,"warning":false}},"headingText":"Overview","containsRefs":false,"markdown":"\n\n\nIn this module, we will learn about logistic regression, which is a type of regression analysis used to model binary outcomes. Logistic regression is used when the dependent variable is a binary variable (0/1, Yes/No, True/False). Logistic regression is a type of generalized linear model (GLM) that uses the logistic function to model the relationship between the dependent variable and one or more independent variables. Logistic regression is widely used in many fields, including epidemiology, economics, and political science, to model binary outcomes.\n\nHere is a great video by Andrew Ng to get you started with the basic intuition behind logistic regression:\n\n{{< video https://youtu.be/xuTiAW0OR40?si=sUoIr5pzHwHEpazO title='Multiple Linear Regression' >}}\n\n\n## Binary Outcomes\n\nSo far, we have looked at continuous or numerical outcomes (response variables). We are often also interested in outcome variables that are binary (Yes/No, or 1/0). For example, did violence happen, or not? Does a country have judicial review? Did a country vote for a particular resolution in the UN? \n\nIn order to model binary outcomes, we need to use a different type of regression model called **logistic regression**. Logistic regression is used to model the probability of a certain class or event existing such as pass/fail, win/lose, alive/dead or healthy/sick. Sometimes you will see analysts using OLS to model binary outcomes, but this is not usually the most appropriate choice. For a number of reasons, logistic regression is a better choice, including the fact that the predicted values are probabilities (which are always between 0 and 1) and the model is more robust to violations of the assumptions of OLS regression.\n\nWhen modeling binary outcomes, we can think of the outcome as a **Bernoulli trial**. A Bernoulli trial is a random experiment with exactly two possible outcomes, \"success\" and \"failure\", in which the probability of success is the same every time the experiment is conducted. Success is usually coded as 1, failure as 0. So, ironically, something like conflict onset is a \"success\" in this context.\n\nEach Bernoulli trial can have a separate probability of success\n\n$$ y_i ∼ Bern(p) $$\n\nWe can then use the predictor variables to model that probability of success, $p_i$. This is a very general way of addressing many problems in regression and the resulting models are called **generalized linear models (GLMs)**. Logistic regression is a very common example of GLMS but there are others, like probit and Poisson regression, which we will not cover in this course.\n\nAll GLMs have the following three characteristics: a probability distribution describing a generative model for the outcome variable; a linear model; and a link function that relates the linear model to the parameter of the outcome distribution.\n\nThe linear model can be written as follows: \n\n$$\\eta = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_k X_k$$\nAs was mentioned previously, logistic regression is a common GLM used to model a binary categorical outcome (0 or 1). In logistic regression, the link function that connects $\\eta_i$ to $p_i$ is the **logit function** which can be written as follows. For $0\\le p \\le 1$:\n\n$$logit(p) = \\log\\left(\\frac{p}{1-p}\\right)$$\nHere is what that function looks like when we plot it:\n\n```{r}\n#| label: logit_function\n#| echo: false\n\nlibrary(tidyverse)\n\nd <- tibble(p = seq(0.001, 0.999, length.out = 1000)) %>%\n  mutate(logit_p = log(p/(1-p)))\n\nggplot(d, aes(x = p, y = logit_p)) + \n  geom_line() + \n  xlim(0,1) + \n  ylab(\"logit(p)\") +\n  labs(title = \"logit(p) vs. p\")\n```\n\n\nThis plot shows that the logit function is a transformation of the probability $p$ that maps it to the real line. The logit function is useful because it maps the probability of success to the real line, which is necessary for the linear model.\n\n## Analyzing conflict onset\n\nWe can use logit models to analyze conflict onset. In this example, we will use the `peacesciencer` package to analyze the onset of conflict. The `peacesciencer` package provides a number of datasets and functions for analyzing conflict and peace. It provides data from a number of important datasets in the field of conflict studies, e.g. the Correlates of War (CoW) project, the Uppsala Conflict Data Program (UCDP), and the Militarized Interstate Dispute (MID) dataset. It also provides functions for analyzing conflict and adding control variables to the dataset.\n\nLet's go ahead and load the `peacesciencer` package and create a dataset that we can use to analyze conflict onset. We will use the `create_stateyears()` function to create a dataset with state-years, and then add some control variables to the dataset. We will add the UCDP/PRIO Armed Conflict Dataset (ACD) to the dataset, which provides information on the onset of armed conflict. We will also add the Polity IV dataset, which provides information on the level of democracy in each country-year. We will add the CREG fractionalization index, which provides information\n\n```{r}\n#| label: use_peacesciencer\n#| echo: true\n\nlibrary(peacesciencer)\nlibrary(tidymodels)\n\nconflict_df <- create_stateyears(system = 'gw') |>\n  filter(year %in% c(1946:1999)) |>\n  add_ucdp_acd(type=c(\"intrastate\"), only_wars = FALSE) |>\n  add_democracy() |>\n  add_creg_fractionalization() |>\n  add_sdp_gdp() |>\n  add_rugged_terrain()\n\nglimpse(conflict_df)\n```\n\nNow we can use these data to analyze conflict onset using logistic regression. We will use the `glm()` function to fit a very basic binary logistic regression model where we predict conflict onset with GDP per capita. \n\nThe `glm()` function is used to fit generalized linear models, which include logistic regression models. Note that we have to specify the `family` argument as `\"binomial\"` to identify logit as the link function. \n\nWe will use the `ucdponset` variable as the outcome variable, which indicates whether an armed conflict onset occurred in a given year and country. We will use the `wbgdppc2011est` variable as the predictor variable, which indicates the GDP per capita of the country in a given year. Finally we will use the `summary()` function to summarize the results of the logistic regression model.\n\n```{r}\n#| label: bivariate_model\n#| echo: true\n\nconflict_model <- glm(ucdponset ~ wbgdppc2011est, \n                      data= conflict_df,\n                      family = \"binomial\")\n\nsummary(conflict_model)\n```\n\nTo interpret the results of the model there are few things we can do. First, we can just look at the direction and significance of the coefficients, just like we would in a linear regression model. Second, we can exponentiate the coefficients to get the odds ratios. The odds ratio is the ratio of the odds of the outcome occurring in one group to the odds of the outcome occurring in another group. In this case, the odds ratio is the ratio of the odds of conflict onset occurring in one group to the odds of conflict onset occurring in another group. Finally, we can use the coefficients to calculate the probability of the outcome occurring for different values of the predictor variable.\n\nLet's start by looking at the coefficients of the model, namely the coefficient for log GPD per capita. Here we see that the coefficient is -0.33. \n\n\n$$\\log\\left(\\frac{p}{1-p}\\right) = -1.16-0.33\\times \\text{logGDPpc}$$\n\nThis tells us that the relationship between GDP and conflict onset is negative - as GDP per capita increases, the odds of conflict onset decrease. But we cannot interpret the magnitude of the effect directly. For this, we need to exponentiate the coefficient to get the odds ratio. The odds ratio is the ratio of the odds of the outcome occurring in one group to the odds of the outcome occurring in another group. \n\nIf we exponentiate the coefficient, we get an odds ratio of 0.718. This means that for each one unit increase in log GDP per capita, the odds of the outcome occurring are multiplied by approximately 0.718, assuming other variables in the model are held constant.\n\nThis means that an increase in GDP per capita is associated with a **decrease** in the odds of the outcome occurring. The odds of the outcome decrease by about 28.2% for each unit increase in GDP per capita (on average).\n\nAnother thing we can do is calculate the probability of conflict onset for different values of GDP per capita. We can do this by calculating the predicted probabilities for different values of GDP per capita. One way to do this is to use the `marginaleffects` package, which calculates the marginal effects of a model for different values of the predictor variable.\n\nBelow, we calculate the predicted probability of conflict onset for different countries with very different levels of GDP--the United States, Venezuela and Rwanda. We start by selecting the data for these countries in 1999, and then we calculate the marginal effects of the model for these countries by calling the `predictions()` function from the `marginaleffects` package. Finally, we clean up the results with the `tidy()` function from the `broom` package to make them easier to interpret.\n\n```{r}\n#| label: marginal_effects\n\n# load the marginaleffects library\nlibrary(marginaleffects)\n\n# select some countries for a given year\nselected_countries <- conflict_df |>\n  filter(\n    statename %in% c(\"United States of America\", \"Venezuela\", \"Rwanda\"),\n    year == 1999)\n\n# calculate margins for the subset\nmarg_effects <- predictions(conflict_model, newdata = selected_countries)\n\n# tidy the results\ntidy(marg_effects) |>\n  select(estimate, p.value, conf.low, conf.high, statename)\n```\n\nThe results show that the predicted probability of conflict onset is highest for Rwanda, followed by Venezuela, and lowest for the United States. This is consistent with the idea that GDP per capita is negatively associated with conflict onset.","srcMarkdownNoYaml":"\n\n## Overview\n\nIn this module, we will learn about logistic regression, which is a type of regression analysis used to model binary outcomes. Logistic regression is used when the dependent variable is a binary variable (0/1, Yes/No, True/False). Logistic regression is a type of generalized linear model (GLM) that uses the logistic function to model the relationship between the dependent variable and one or more independent variables. Logistic regression is widely used in many fields, including epidemiology, economics, and political science, to model binary outcomes.\n\nHere is a great video by Andrew Ng to get you started with the basic intuition behind logistic regression:\n\n{{< video https://youtu.be/xuTiAW0OR40?si=sUoIr5pzHwHEpazO title='Multiple Linear Regression' >}}\n\n\n## Binary Outcomes\n\nSo far, we have looked at continuous or numerical outcomes (response variables). We are often also interested in outcome variables that are binary (Yes/No, or 1/0). For example, did violence happen, or not? Does a country have judicial review? Did a country vote for a particular resolution in the UN? \n\nIn order to model binary outcomes, we need to use a different type of regression model called **logistic regression**. Logistic regression is used to model the probability of a certain class or event existing such as pass/fail, win/lose, alive/dead or healthy/sick. Sometimes you will see analysts using OLS to model binary outcomes, but this is not usually the most appropriate choice. For a number of reasons, logistic regression is a better choice, including the fact that the predicted values are probabilities (which are always between 0 and 1) and the model is more robust to violations of the assumptions of OLS regression.\n\nWhen modeling binary outcomes, we can think of the outcome as a **Bernoulli trial**. A Bernoulli trial is a random experiment with exactly two possible outcomes, \"success\" and \"failure\", in which the probability of success is the same every time the experiment is conducted. Success is usually coded as 1, failure as 0. So, ironically, something like conflict onset is a \"success\" in this context.\n\nEach Bernoulli trial can have a separate probability of success\n\n$$ y_i ∼ Bern(p) $$\n\nWe can then use the predictor variables to model that probability of success, $p_i$. This is a very general way of addressing many problems in regression and the resulting models are called **generalized linear models (GLMs)**. Logistic regression is a very common example of GLMS but there are others, like probit and Poisson regression, which we will not cover in this course.\n\nAll GLMs have the following three characteristics: a probability distribution describing a generative model for the outcome variable; a linear model; and a link function that relates the linear model to the parameter of the outcome distribution.\n\nThe linear model can be written as follows: \n\n$$\\eta = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_k X_k$$\nAs was mentioned previously, logistic regression is a common GLM used to model a binary categorical outcome (0 or 1). In logistic regression, the link function that connects $\\eta_i$ to $p_i$ is the **logit function** which can be written as follows. For $0\\le p \\le 1$:\n\n$$logit(p) = \\log\\left(\\frac{p}{1-p}\\right)$$\nHere is what that function looks like when we plot it:\n\n```{r}\n#| label: logit_function\n#| echo: false\n\nlibrary(tidyverse)\n\nd <- tibble(p = seq(0.001, 0.999, length.out = 1000)) %>%\n  mutate(logit_p = log(p/(1-p)))\n\nggplot(d, aes(x = p, y = logit_p)) + \n  geom_line() + \n  xlim(0,1) + \n  ylab(\"logit(p)\") +\n  labs(title = \"logit(p) vs. p\")\n```\n\n\nThis plot shows that the logit function is a transformation of the probability $p$ that maps it to the real line. The logit function is useful because it maps the probability of success to the real line, which is necessary for the linear model.\n\n## Analyzing conflict onset\n\nWe can use logit models to analyze conflict onset. In this example, we will use the `peacesciencer` package to analyze the onset of conflict. The `peacesciencer` package provides a number of datasets and functions for analyzing conflict and peace. It provides data from a number of important datasets in the field of conflict studies, e.g. the Correlates of War (CoW) project, the Uppsala Conflict Data Program (UCDP), and the Militarized Interstate Dispute (MID) dataset. It also provides functions for analyzing conflict and adding control variables to the dataset.\n\nLet's go ahead and load the `peacesciencer` package and create a dataset that we can use to analyze conflict onset. We will use the `create_stateyears()` function to create a dataset with state-years, and then add some control variables to the dataset. We will add the UCDP/PRIO Armed Conflict Dataset (ACD) to the dataset, which provides information on the onset of armed conflict. We will also add the Polity IV dataset, which provides information on the level of democracy in each country-year. We will add the CREG fractionalization index, which provides information\n\n```{r}\n#| label: use_peacesciencer\n#| echo: true\n\nlibrary(peacesciencer)\nlibrary(tidymodels)\n\nconflict_df <- create_stateyears(system = 'gw') |>\n  filter(year %in% c(1946:1999)) |>\n  add_ucdp_acd(type=c(\"intrastate\"), only_wars = FALSE) |>\n  add_democracy() |>\n  add_creg_fractionalization() |>\n  add_sdp_gdp() |>\n  add_rugged_terrain()\n\nglimpse(conflict_df)\n```\n\nNow we can use these data to analyze conflict onset using logistic regression. We will use the `glm()` function to fit a very basic binary logistic regression model where we predict conflict onset with GDP per capita. \n\nThe `glm()` function is used to fit generalized linear models, which include logistic regression models. Note that we have to specify the `family` argument as `\"binomial\"` to identify logit as the link function. \n\nWe will use the `ucdponset` variable as the outcome variable, which indicates whether an armed conflict onset occurred in a given year and country. We will use the `wbgdppc2011est` variable as the predictor variable, which indicates the GDP per capita of the country in a given year. Finally we will use the `summary()` function to summarize the results of the logistic regression model.\n\n```{r}\n#| label: bivariate_model\n#| echo: true\n\nconflict_model <- glm(ucdponset ~ wbgdppc2011est, \n                      data= conflict_df,\n                      family = \"binomial\")\n\nsummary(conflict_model)\n```\n\nTo interpret the results of the model there are few things we can do. First, we can just look at the direction and significance of the coefficients, just like we would in a linear regression model. Second, we can exponentiate the coefficients to get the odds ratios. The odds ratio is the ratio of the odds of the outcome occurring in one group to the odds of the outcome occurring in another group. In this case, the odds ratio is the ratio of the odds of conflict onset occurring in one group to the odds of conflict onset occurring in another group. Finally, we can use the coefficients to calculate the probability of the outcome occurring for different values of the predictor variable.\n\nLet's start by looking at the coefficients of the model, namely the coefficient for log GPD per capita. Here we see that the coefficient is -0.33. \n\n\n$$\\log\\left(\\frac{p}{1-p}\\right) = -1.16-0.33\\times \\text{logGDPpc}$$\n\nThis tells us that the relationship between GDP and conflict onset is negative - as GDP per capita increases, the odds of conflict onset decrease. But we cannot interpret the magnitude of the effect directly. For this, we need to exponentiate the coefficient to get the odds ratio. The odds ratio is the ratio of the odds of the outcome occurring in one group to the odds of the outcome occurring in another group. \n\nIf we exponentiate the coefficient, we get an odds ratio of 0.718. This means that for each one unit increase in log GDP per capita, the odds of the outcome occurring are multiplied by approximately 0.718, assuming other variables in the model are held constant.\n\nThis means that an increase in GDP per capita is associated with a **decrease** in the odds of the outcome occurring. The odds of the outcome decrease by about 28.2% for each unit increase in GDP per capita (on average).\n\nAnother thing we can do is calculate the probability of conflict onset for different values of GDP per capita. We can do this by calculating the predicted probabilities for different values of GDP per capita. One way to do this is to use the `marginaleffects` package, which calculates the marginal effects of a model for different values of the predictor variable.\n\nBelow, we calculate the predicted probability of conflict onset for different countries with very different levels of GDP--the United States, Venezuela and Rwanda. We start by selecting the data for these countries in 1999, and then we calculate the marginal effects of the model for these countries by calling the `predictions()` function from the `marginaleffects` package. Finally, we clean up the results with the `tidy()` function from the `broom` package to make them easier to interpret.\n\n```{r}\n#| label: marginal_effects\n\n# load the marginaleffects library\nlibrary(marginaleffects)\n\n# select some countries for a given year\nselected_countries <- conflict_df |>\n  filter(\n    statename %in% c(\"United States of America\", \"Venezuela\", \"Rwanda\"),\n    year == 1999)\n\n# calculate margins for the subset\nmarg_effects <- predictions(conflict_model, newdata = selected_countries)\n\n# tidy the results\ntidy(marg_effects) |>\n  select(estimate, p.value, conf.low, conf.high, statename)\n```\n\nThe results show that the predicted probability of conflict onset is highest for Rwanda, followed by Venezuela, and lowest for the United States. This is consistent with the idea that GDP per capita is negatively associated with conflict onset."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"highlight-style":"atom-one","output-file":"module-6.1.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.533","editor":"source","theme":{"light":"cosmo","dark":"cyborg"},"mainfont":"Atkinson Hyperlegible","code-copy":true,"title":"Module 6.1","subtitle":"Logistic Regression"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}